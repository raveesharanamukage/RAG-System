{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTGOXrqnFePAqHsnlwKQ4+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raveesharanamukage/RAG-System/blob/main/RAG_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsqb8znlnMtB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Retrieval-Augmented Generation (RAG)**\n",
        "\n",
        "RAG is a hybrid approach combining retrieval and generation to enhance the capabilities of large language models (LLMs)\n",
        "\n",
        "+ **Retrieval**: Fetching relevant documents or text chunks from a knowledge base based on a user’s query.\n",
        "\n",
        "+ **Augmentation**: Using the retrieved information to augment the prompt sent to the LLM.\n",
        "\n",
        "+ **Generation**: Generating a response using the LLM, informed by the augmented context.\n",
        "\n",
        "RAG is particularly useful for grounding LLMs in specific, external knowledge, reducing hallucination (when models generate incorrect or fabricated information) and enabling them to answer questions based on provided documents.\n",
        "\n",
        "**2. Vector Embeddings**\n",
        "\n",
        "Text data is not directly comparable in a meaningful way for similarity searches. Vector embeddings are numerical representations of text in a high-dimensional space, where semantically similar texts are mapped to nearby points. These embeddings are generated using models like BERT or, in this case, Mistral’s mistral-embed.\n",
        "\n",
        "\n",
        "\n",
        "*   **How they work:** A pre-trained neural network transforms text into a fixed-length vector (e.g., 1024 dimensions in this code).\n",
        "\n",
        "*   **Use case:** Embeddings allow similarity searches by computing distances (e.g., Euclidean distance) between vectors.\n",
        "\n",
        "**3.Vector Database and FAISS**\n",
        "\n",
        "A vector database **stores embeddings** and enables efficient **similarity searches**.\n",
        "**FAISS** (Facebook AI Similarity Search) is a library designed for fast nearest-neighbor searches in high-dimensional spaces.\n",
        "\n",
        "+ **IndexFlatL2:** This is a simple FAISS index that uses L2 (Euclidean) distance to measure similarity between vectors. It’s suitable for small datasets, as it performs an exact search without approximations.\n",
        "\n",
        "+ **Search:** Given a query embedding, FAISS returns the indices of the most similar embeddings in the database.\n",
        "\n",
        "**4.Mistral AI**\n",
        "Mistral AI provides models for both **text generation (mistral-small-2503)** and **embedding generation (mistral-embed)**. The code uses Mistral’s API to:\n",
        "\n",
        "+ Generate embeddings for text chunks and queries.\n",
        "+ Generate responses to augmented prompts.\n",
        "\n",
        "**5. Chunking**\n",
        "\n",
        "Large documents are often too long to process in one go due to model input limits or computational constraints. Chunking involves splitting text into smaller, manageable pieces (e.g., 2068 characters in this code) while preserving meaning as much as possible.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UpH_uaRUYlPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai import Mistral\n",
        "import requests\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "api_key=getpass(\"Enter the API key:\")\n",
        "client=Mistral(api_key=api_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVPRjJ7eoMT3",
        "outputId": "110d4036-39cd-4eff-df71-ca3a0e2b7f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"mistral-small-2503\"\n",
        "chat_response=client.chat.complete(model=model_id,messages=[\n",
        "    {\n",
        "    \"role\":\"user\",\n",
        "    \"content\":\"what is the meaning of life?\"\n",
        "}\n",
        "    ]\n",
        "                                   )\n",
        "print(chat_response.choices[0].message.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwPN4AB3pDR8",
        "outputId": "a08ccefd-04f2-4b09-e67b-8aa395edb388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The meaning of life is a philosophical question that has been debated for centuries, and it doesn't have one definitive answer as it can vary greatly depending on personal beliefs, religious or spiritual views, and philosophical persuasions. Here are a few perspectives:\n",
            "\n",
            "1. **Existentialism**: Existentialists like Jean-Paul Sartre argued that life has no inherent meaning, and it's up to each individual to create their own purpose.\n",
            "\n",
            "2. **Religious and Spiritual Views**: Many religions provide their own answers. For example:\n",
            "   - In Christianity, the purpose of life might be seen as loving and serving God.\n",
            "   - In Buddhism, the purpose might be achieving enlightenment.\n",
            "\n",
            "3. **Hedonism**: Hedonists believe the purpose of life is to seek pleasure and happiness.\n",
            "\n",
            "4. **Altruism**: Some people find meaning in life through helping others and making the world a better place.\n",
            "\n",
            "5. **Personal Growth**: Many people find purpose in learning, growing, and becoming the best version of themselves.\n",
            "\n",
            "6. **Biological Perspective**: From a scientific or biological standpoint, one might say the purpose of life is to survive and reproduce, ensuring the continuation of the species.\n",
            "\n",
            "7. **Simplistic Views**: Some people believe the meaning of life is simply to live, to experience, and to be.\n",
            "\n",
            "8. **The Answer from \"The Hitchhiker's Guide to the Galaxy\"**: In this book by Douglas Adams, a group of hyper-intelligent pan-dimensional beings build a supercomputer named Deep Thought to calculate the answer to the Ultimate Question of Life, The Universe, and Everything, to which Deep Thought famously replies \"42,\" though it's important to note that the characters later realize they don't actually know what the Ultimate Question is.\n",
            "\n",
            "Ultimately, the meaning of life may be a deeply personal and subjective concept. It could be helpful to reflect on what gives your life purpose and what you value most.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=requests.get(\"https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\")\n",
        "text=response.text\n",
        "f=open('essay.txt','w')\n",
        "f.write(text)\n",
        "f.close"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0mnQ3mjyKpg",
        "outputId": "f5763997-3add-4348-829b-31c71f0b337d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function TextIOWrapper.close()>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2gG5SXS1xDS",
        "outputId": "32aca7ca-e4c3-4f53-e802-9245e471adaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "75014"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking the data"
      ],
      "metadata": {
        "id": "gHr_w9oW11Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size=2068\n",
        "chunks=[text[i:i+chunk_size] for i in range(0,len(text),chunk_size) ]\n"
      ],
      "metadata": {
        "id": "3RN9-iXe1zUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwCr0UNw23PB",
        "outputId": "2e5eea4f-9b38-486a-c958-32e0b1867853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Numerical Representation of the Data"
      ],
      "metadata": {
        "id": "Xg0dNOIj293J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_embeddings(input):\n",
        "  embeddings_batch_response=client.embeddings.create(\n",
        "      model=\"mistral-embed\",\n",
        "      inputs=input\n",
        "  )\n",
        "  return embeddings_batch_response.data[0].embedding"
      ],
      "metadata": {
        "id": "7vx_8kkL281R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_text_embeddings(chunks[0])"
      ],
      "metadata": {
        "id": "woXeQ5jP334C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_embedding_batch(batch):\n",
        "  embedding_batch_response=client.embeddings.create(\n",
        "      model=\"mistral-embed\",\n",
        "      inputs=batch\n",
        "  )\n",
        "  return [embedding_batch_response.data[i].embedding for i in range(len(batch))]\n",
        "\n"
      ],
      "metadata": {
        "id": "JCguFk9t5ce7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings=np.array(get_text_embedding_batch(chunks))"
      ],
      "metadata": {
        "id": "mjw6za5D4Qxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L0NrLKA5DlN",
        "outputId": "ddc35bd7-24fe-49ff-9dd8-185717bc230a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(37, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing(Populating the Vector DB with Data Chunks and its embeddings)"
      ],
      "metadata": {
        "id": "IPEehN0J7wN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d=text_embeddings.shape[1]\n",
        "index=faiss.IndexFlatL2(d)\n",
        "index.add(text_embeddings)"
      ],
      "metadata": {
        "id": "MQfa0Ojc7ruQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What were the two main things the author worked on before college?\"\n",
        "question_embedding=np.array([get_text_embeddings(question)])\n",
        "question_embedding.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrSRznPb9Z6m",
        "outputId": "5c3a11a9-ac0c-42b6-a85c-5ebbda080f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdZic10b_Wv9",
        "outputId": "bdc4279c-2fa9-49f5-efd7-1016d9d2f9d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.05447388,  0.03479004,  0.0375061 , ..., -0.02787781,\n",
              "        -0.00327492,  0.0029068 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding most Similar Chunks(Retrieval)"
      ],
      "metadata": {
        "id": "lRM7Zaiv_t4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D,I=index.search(question_embedding,k=2)\n",
        "print(I)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuCIGWMm_tJn",
        "outputId": "3e8c212d-2dca-402f-cf79-676171f0762f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0 32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_chunk=[chunks[i] for i in I.tolist()[0]]\n",
        "print(retrieved_chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPu7T5xGALFl",
        "outputId": "1f58ccd4-47a6-4804-c161-f813d6a73a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could r', \"It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\\n\\nIn the fall of 2019, Bel was finally finished. Like McCarthy's original Lisp, it's a spec rather than an implementation, although like McCarthy's Lisp it's a spec expressed as code.\\n\\nNow that I could write essays again, I wrote a bunch about topics I'd had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who'd lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotes\\n\\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.\\n\\n[2] Italian words for abstract concepts can nearly always be predicted from their English cognates (except for occasional traps like polluzione). It's the everyday words that differ. So if you string together a lot of abstract concepts with a few simple verbs, you can make a little Italian go a long way.\\n\\n[3] I lived at Piazza San Felice 4, so my walk to the Accademia went straight down the spine of old Florence: past the Pitti, across the bridge, past Orsanmichele, between the Duomo and the Baptistery, and \"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Augmentation"
      ],
      "metadata": {
        "id": "WoufAz8OA0gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Context information is below,\n",
        "----------------------------\n",
        "{retrieved_chunk}\n",
        "---------------------------\n",
        "Given the context information and not prior knowledge,answer the query,\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "MMdNPXXdAx9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_mistral(user_message,model=\"mistral-small-2503\"):\n",
        "  messages=[\n",
        "      {\n",
        "      \"role\":\"user\",\n",
        "      \"content\":user_message\n",
        "      }\n",
        "      ]\n",
        "  chat_response=client.chat.complete(\n",
        "      model=model,\n",
        "      messages=messages\n",
        "  )\n",
        "\n",
        "  return(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "hUnn4eSRBfbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " run_mistral(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uj4mdRL5CT-L",
        "outputId": "22d3da1d-3b71-4c67-83b2-b319d8018324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Before college, the author worked on two main things outside of school: writing and programming. Specifically, the author wrote short stories and began programming on an IBM 1401 using an early version of Fortran.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#without knowledge base\n",
        "run_mistral(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YOsIYNMFDjg0",
        "outputId": "9bec4c2b-e980-449d-887e-1b29c3d30e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A. The author worked on a farm and did a lot of reading.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}